import streamlit as st
import os 
from google import genai
from google.genai.errors import APIError
from google.genai.types import Content, Part

# --- 1. Configura√ß√£o da Interface ---
st.set_page_config(page_title="Mentor de Carreira PDI (Gemini)", page_icon="üéØ", layout="centered")

st.title("üéØ Mentor de PDI Inteligente (Gemini)")
st.markdown("Ol√°! Sou seu assistente de carreira. Vamos construir seu **Plano de Desenvolvimento Individual** juntos.")

# --- CSS para Layout Preto/Branco e Estabilidade ---
st.markdown("""
<style>
    /* 1. Estilos de Cores */
    .stApp {background-color: #000000; color: #FFFFFF;}
    h1, h2, h3, h4, p, .stMarkdown {color: #FFFFFF !important;}
    
    /* 2. Largura e Padding */
    .block-container {padding-top: 2rem; padding-bottom: 0rem; padding-left: 2rem; padding-right: 2rem; max-width: 800px;}
    
    /* 3. Estilo das Caixas de Mensagem */
    .stChatMessage {border-radius: 15px; padding: 15px; background-color: #1A1A1A; color: #FFFFFF !important; border: 1px solid #444444;}
    
    /* 4. Estilo da Barra de Input de Mensagem */
    .stTextInput > div > div > input, .stTextInput > label {
        color: #FFFFFF; background-color: #000000; border: 1px solid #FFFFFF; border-radius: 8px;
    }
    
    /* 5. CORRE√á√ÉO DE LEGIBILIDADE PARA ST.RADIO E ST.SELECT */
    .stRadio > label, .stRadio > div > label > div > div > p {
        color: #FFFFFF !important; 
    }
    
    /* 6. Oculta st.button simples (s√≥ aparecem os de formul√°rio) */
    .stButton>button {display: none;}
    
    /* 7. OCULTA BARRAS DE CABE√áALHO E RODAP√â */
    header {visibility: hidden; height: 0px;}
    footer {visibility: hidden; height: 0px;}
    #MainMenu {visibility: hidden;}
    
    /* 8. Estilo do bot√£o de formul√°rio para que ele apare√ßa */
    div.stButton > button {
        display: inline-block; 
        color: white; 
        background-color: #4A90E2; 
        border: none;
        border-radius: 5px; 
        padding: 10px 15px;
        cursor: pointer;
    }
</style>
""", unsafe_allow_html=True)


# --- 2. Vari√°veis de Estado e Perguntas PERSONALIZADAS ---
QUESTION_FLOW = [
    # Bloco 1: Configura√ß√µes (st.radio)
    {"type": "intro", "text": "Antes de come√ßarmos, vamos configurar o **idioma e o estilo de resposta** do nosso Mentor. Isso garante uma comunica√ß√£o perfeita!"},
    {"type": "select", "question": "Em qual idioma voc√™ prefere que o Mentor de PDI responda?", 
     "key": "lang", "options": ["Portugu√™s", "Ingl√™s", "Espanhol"]},
    {"type": "select", "question": "Qual estilo de intera√ß√£o voc√™ prefere?", 
     "key": "style", "options": ["Extrovertido", "Profissional"]},
    {"type": "select", "question": "Voc√™ prefere respostas com mais ou menos detalhes?", 
     "key": "detail", "options": ["Muito Detalhe", "Direto ao Ponto"]},

    # Bloco 2: Sobre Voc√™ (st.chat_input)
    {"type": "intro", "text": "√ìtimo! Agora, come√ßarei fazendo algumas perguntas sobre voc√™. Tudo bem?"},
    {"type": "input", "question": "Como voc√™ preferiria que eu te chamasse?"},
    {"type": "input", "question": "Quantos anos voc√™ tem?"},

    # Bloco 3: Experi√™ncias Educacionais (st.chat_input)
    {"type": "intro", "text": "Perfeito. Agora, gostaria de explorarmos mais detalhes sobre suas **experi√™ncias educacionais**."},
    {"type": "input", "question": "Qual foi o maior n√≠vel de educa√ß√£o que voc√™ j√° obteve? (Ex: Bacharelado, Mestrado, P√≥s-doutorado)"},
    {"type": "input", "question": "Em qual institui√ß√£o voc√™ obteve essa forma√ß√£o?"},
    {"type": "input", "question": "Qual foi a sua √°rea de estudo?"},

    # Bloco 4: Experi√™ncia Profissional (st.chat_input)
    {"type": "intro", "text": "Entendido. Vamos agora para o bloco de **experi√™ncia profissional**."},
    {"type": "input", "question": "Voc√™ j√° trabalhou como jovem aprendiz? Se sim, em qual ano foi sua primeira experi√™ncia nesse formato?"},
    {"type": "input", "question": "Voc√™ j√° trabalhou como estagi√°rio(a)? Se sim, em qual ano foi sua primeira experi√™ncia nesse formato?"},
    {"type": "input", "question": "Voc√™ j√° trabalhou como funcion√°rio CLT? Se sim, em qual ano foi sua primeira experi√™ncia nesse formato?"},
    {"type": "input", "question": "Por favor, cite os nomes das empresas nas quais voc√™ j√° trabalhou como CLT (separe por v√≠rgulas)"},
    {"type": "input", "question": "Voc√™ est√° trabalhando atualmente? Se sim, cite qual √© o nome da sua posi√ß√£o e empresa atuais"},

    # Bloco 5: Objetivos Profissionais (st.chat_input)
    {"type": "intro", "text": "Para finalizar nosso formul√°rio, vamos focar nos seus **objetivos profissionais**."},
    {"type": "input", "question": "Quais s√£o os seus principais objetivos profissionais?"}
]
NUM_FLOW_STEPS = len(QUESTION_FLOW)

# --- 3. Carregamento Secreto da Chave ---
gemini_api_key = os.environ.get("GEMINI_API_KEY")

# --- 4. L√≥gica de Mem√≥ria (Hist√≥rico e Estado) ---
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "system", "content": ""}] 
    st.session_state.pdi_state = 0 
    st.session_state.configs = {} 

# Fun√ß√£o que executa o submit do formul√°rio de sele√ß√£o
def submit_form(key, question):
    selected_option = st.session_state[f'select_{st.session_state.pdi_state}']

    # 1. Armazena a configura√ß√£o
    st.session_state.configs[key] = selected_option
    
    # 2. Registra a resposta do usu√°rio no hist√≥rico
    st.session_state.messages.append({"role": "user", "content": f"{question}: {selected_option}"})
    
    # 3. Avan√ßa o estado e for√ßa a reexecu√ß√£o
    st.session_state.pdi_state += 1 
    st.rerun() 


# Fun√ß√£o para montar o System Prompt baseado nas configura√ß√µes
def build_system_prompt():
    lang = st.session_state.configs.get('lang', 'Portugu√™s')
    style = st.session_state.configs.get('style', 'Profissional')
    detail = st.session_state.configs.get('detail', 'Muito Detalhe')
    
    return f"""
        Voc√™ √© um Mentor de Carreira S√™nior especializado em criar Planos de Desenvolvimento Individual (PDI).
        
        INSTRU√á√ïES DE COMPORTAMENTO R√çGIDAS:
        1. EDUCA√á√ÉO: Voc√™ **DEVE** ser sempre cort√™s, educado e profissional. **NUNCA** use linguagem passivo-agressiva ou grosseira, mesmo ao pedir esclarecimentos ou ao criticar objetivos.
        2. IDIOMA PRINCIPAL: Responda APENAS em {lang}.
        3. TOM E DETALHE: O tom de voz deve ser {style} e o n√≠vel de profundidade deve ser {detail}. Se for 'Direto ao Ponto', use listas e par√°grafos curtos.
        
        SUA MISS√ÉO:
        Voc√™ acaba de receber as respostas iniciais do usu√°rio.
        
        1. REVISE E VALIDE: Revise as respostas com o tom {style}. Se alguma informa√ß√£o crucial parecer incompleta, pe√ßa esclarecimento de forma **educada**.
        2. INICIE A AN√ÅLISE: Ap√≥s a valida√ß√£o, comece a etapa 2 do PDI: 'Identificar Gaps (O que falta aprender?)'. Baseie-se nas experi√™ncias e objetivos.
        """

# Fun√ß√£o para gerar o conte√∫do usando o Gemini
def generate_gemini_response(prompt, api_key):
    st.session_state.messages[0]['content'] = build_system_prompt()
    system_prompt = st.session_state.messages[0]['content']

    if not api_key: st.error("Erro de configura√ß√£o: A chave GEMINI_API_KEY n√£o foi encontrada."); return None
        
    try:
        client = genai.Client(api_key=api_key)
        
        history_messages = []
        for m in st.session_state.messages[1:]:
            role = 'user' if m['role'] == 'user' else 'model'
            content_obj = Content(role=role, parts=[Part.from_text(text=m['content'])]) 
            history_messages.append(content_obj)
        
        history_messages.append(Content(role='user', parts=[Part.from_text(text=prompt)]))

        response = client.models.generate_content(
            model='gemini-2.5-flash', 
            contents=history_messages, 
            config={'system_instruction': system_prompt} 
        )
        return response
    
    except APIError as e: st.error(f"Erro na API do Gemini: Detalhe: {e}"); return None
    except Exception as e: st.error(f"Ocorreu um erro inesperado: {e}"); return None


# --- 5. L√≥gica da M√°quina de Estados (Controle do Fluxo) ---

# Exibir mensagens anteriores no chat
for msg in st.session_state.messages:
    # Ignoramos a mensagem do "system" (√≠ndice 0)
    if msg["role"] != "system":
        role = 'assistant' if msg["role"] == 'model' else msg["role"]
        # Se a mensagem foi salva, ela ser√° exibida aqui
        st.chat_message(role).write(msg["content"])


# L√≥gica para avan√ßar o formul√°rio ou iniciar o chat
if st.session_state.pdi_state < NUM_FLOW_STEPS:
    
    current_step = QUESTION_FLOW[st.session_state.pdi_state]
    
    # 5.1. Exibir Introdu√ß√£o E SALVAR NO HIST√ìRICO
    if current_step["type"] == "intro":
        # A mensagem √© exibida para o usu√°rio (como assistente)
        st.chat_message("assistant").write(current_step["text"])
        
        # A MENSAGEM √â SALVA NO HIST√ìRICO PARA EXIBI√á√ÉO FUTURA
        st.session_state.messages.append({"role": "model", "content": current_step["text"]})
        
        st.session_state.pdi_state += 1
        st.rerun()

    # 5.2. Exibir M√∫ltipla Escolha (st.radio) E SALVAR PERGUNTA NO HIST√ìRICO
    elif current_step["type"] == "select":
        # A pergunta √© exibida para o usu√°rio (como assistente)
        st.chat_message("assistant").write(current_step["question"])
        
        # A PERGUNTA √â SALVA NO HIST√ìRICO
        # (A resposta ser√° salva dentro da submit_form)
        st.session_state.messages.append({"role": "model", "content": current_step["question"]})

        with st.form(key=f'form_{st.session_state.pdi_state}'):
            st.radio("Selecione uma op√ß√£o:", 
                     current_step["options"], 
                     key=f'select_{st.session_state.pdi_state}')
            
            st.form_submit_button(
                "Confirmar e Continuar", 
                on_click=submit_form, 
                kwargs={'key': current_step["key"], 'question': current_step["question"]}
            )
        
        st.stop() 

    # 5.3. Exibir Pergunta de Texto (st.chat_input) E SALVAR NO HIST√ìRICO
    elif current_step["type"] == "input":
        # A pergunta √© exibida para o usu√°rio (como assistente)
        st.chat_message("assistant").write(current_step["question"])
        
        # A PERGUNTA √â SALVA NO HIST√ìRICO
        st.session_state.messages.append({"role": "model", "content": current_step["question"]})
        # A l√≥gica para capturar a resposta est√° abaixo, no st.chat_input


# 5.4. Captura a intera√ß√£o do usu√°rio (apenas para type="input")
if prompt := st.chat_input("Digite sua resposta aqui..."):
    
    # A resposta do usu√°rio j√° √© salva aqui
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    if st.session_state.pdi_state < NUM_FLOW_STEPS:
        # Aumenta o estado se a resposta foi uma entrada de texto
        st.session_state.pdi_state += 1
        
        if st.session_state.pdi_state < NUM_FLOW_STEPS:
            st.rerun() 
        else:
            # Transi√ß√£o final para o Chat Ativo
            with st.chat_message("assistant"):
                st.markdown("‚úÖ **Formul√°rio inicial completo!** O Mentor de Carreira j√° est√° analisando suas respostas. Por favor, aguarde enquanto ele processa a primeira an√°lise e inicia a fase de identifica√ß√£o de *Gaps*.")
                
            final_prompt_to_gemini = st.session_state.messages[-1]['content']
            
            with st.chat_message("assistant"):
                response = generate_gemini_response(final_prompt_to_gemini, gemini_api_key)
                if response:
                    full_response = response.text
                    st.markdown(full_response)
                    st.session_state.messages.append({"role": "model", "content": full_response})
                else:
                    st.session_state.messages.pop()
    else:
        # 5.5. Chat Ativo (Gemini assume)
        with st.chat_message("assistant"):
            response = generate_gemini_response(prompt, gemini_api_key)
            
            if response:
                full_response = response.text
                st.markdown(full_response)
                
                st.session_state.messages.append({"role": "model", "content": full_response})
            else:
                st.session_state.messages.pop()
