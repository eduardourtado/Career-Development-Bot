import streamlit as st
import os 
from google import genai
from google.genai.errors import APIError
from google.genai.types import Content, Part

# --- 1. Configura√ß√£o da Interface ---
st.set_page_config(page_title="Mentor de Carreira PDI (Gemini)", page_icon="üéØ", layout="centered")

st.title("üéØ Mentor de PDI Inteligente (Gemini)")
st.markdown("Ol√°! Sou seu assistente de carreira. Vamos construir seu **Plano de Desenvolvimento Individual** juntos.")

# --- CSS para Layout Preto/Branco (Mantido) ---
st.markdown("""
<style>
    .stApp {background-color: #000000; color: #FFFFFF;}
    h1, h2, h3, h4, p, .stMarkdown {color: #FFFFFF !important;}
    .block-container {padding-top: 2rem; padding-bottom: 0rem; padding-left: 2rem; padding-right: 2rem; max-width: 800px;}
    .stChatMessage {border-radius: 15px; padding: 15px; background-color: #1A1A1A; color: #FFFFFF !important; border: 1px solid #444444;}
    .stTextInput > div > div > input, .stTextInput > label {
        color: #FFFFFF; background-color: #000000; border: 1px solid #FFFFFF; border-radius: 8px;
    }
    .stButton>button {display: none;} /* Oculta todos os st.button simples */
    header {visibility: hidden; height: 0px;}
    footer {visibility: hidden; height: 0px;}
    #MainMenu {visibility: hidden;}
</style>
""", unsafe_allow_html=True)


# --- 2. Vari√°veis de Estado e Perguntas PERSONALIZADAS ---

# Lista de perguntas e introdu√ß√µes para serem feitas sequencialmente.
# O tipo 'input' usa st.chat_input, o tipo 'select' usa st.radio.
QUESTION_FLOW = [
    # Bloco 1: Configura√ß√µes (st.radio)
    {"type": "intro", "text": "Antes de come√ßarmos, vamos configurar o **idioma e o estilo de resposta** do nosso Mentor. Isso garante uma comunica√ß√£o perfeita!"},
    {"type": "select", "question": "Em qual idioma voc√™ prefere que o Mentor de PDI responda? (Obrigat√≥rio para o System Prompt)", 
     "key": "lang", "options": ["Portugu√™s", "Ingl√™s", "Espanhol"]},
    {"type": "select", "question": "Qual tom/estilo de resposta voc√™ prefere do seu Mentor?", 
     "key": "style", "options": ["Profissional e Objetivo", "Emp√°tico e Encorajador", "Direto e Desafiador"]},

    # Bloco 2: Sobre Voc√™ (st.chat_input)
    {"type": "intro", "text": "√ìtimo! Agora, come√ßarei fazendo algumas perguntas sobre voc√™. Tudo bem?"},
    {"type": "input", "question": "Como voc√™ preferiria que eu te chamasse?"},
    {"type": "input", "question": "Quantos anos voc√™ tem?"},

    # Bloco 3: Experi√™ncias Educacionais (st.chat_input)
    {"type": "intro", "text": "Perfeito. Agora, gostaria de explorarmos mais detalhes sobre suas **experi√™ncias educacionais**."},
    {"type": "input", "question": "Qual foi o maior n√≠vel de educa√ß√£o que voc√™ j√° obteve? (Ex: Bacharelado, Mestrado, P√≥s-doutorado)"},
    {"type": "input", "question": "Em qual institui√ß√£o voc√™ obteve essa forma√ß√£o?"},
    {"type": "input", "question": "Qual foi a sua √°rea de estudo?"},

    # Bloco 4: Experi√™ncia Profissional (st.chat_input)
    {"type": "intro", "text": "Entendido. Vamos agora para o bloco de **experi√™ncia profissional**."},
    {"type": "input", "question": "Voc√™ j√° trabalhou como jovem aprendiz? Se sim, em qual ano foi sua primeira experi√™ncia nesse formato?"},
    {"type": "input", "question": "Voc√™ j√° trabalhou como estagi√°rio(a)? Se sim, em qual ano foi sua primeira experi√™ncia nesse formato?"},
    {"type": "input", "question": "Voc√™ j√° trabalhou como funcion√°rio CLT? Se sim, em qual ano foi sua primeira experi√™ncia nesse formato?"},
    {"type": "input", "question": "Por favor, cite os nomes das empresas nas quais voc√™ j√° trabalhou como CLT (separe por v√≠rgulas)"},
    {"type": "input", "question": "Voc√™ est√° trabalhando atualmente? Se sim, cite qual √© o nome da sua posi√ß√£o e empresa atuais"},

    # Bloco 5: Objetivos Profissionais (st.chat_input)
    {"type": "intro", "text": "Para finalizar nosso formul√°rio, vamos focar nos seus **objetivos profissionais**."},
    {"type": "input", "question": "Quais s√£o os seus principais objetivos profissionais?"}
]
NUM_FLOW_STEPS = len(QUESTION_FLOW)

# --- 3. Carregamento Secreto da Chave ---
gemini_api_key = os.environ.get("GEMINI_API_KEY")

# --- 4. L√≥gica de Mem√≥ria (Hist√≥rico) ---
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "system", "content": ""}] # System prompt ser√° montado
    st.session_state.pdi_state = 0 
    st.session_state.configs = {} # Armazena as respostas de select (idioma, estilo)


# Fun√ß√£o para montar o System Prompt baseado nas configura√ß√µes
def build_system_prompt():
    lang = st.session_state.configs.get('lang', 'Portugu√™s')
    style = st.session_state.configs.get('style', 'Profissional e Objetivo')
    
    # Este prompt ser√° injetado no √≠ndice 0 do hist√≥rico antes da chamada final ao Gemini
    return f"""
        Voc√™ √© um Mentor de Carreira S√™nior especializado em criar Planos de Desenvolvimento Individual (PDI).
        
        INSTRU√á√ïES DE RESPOSTA:
        1. IDIOMA PRINCIPAL: Responda **APENAS em {lang}**, independente do idioma que o usu√°rio usar nas entradas de texto.
        2. TOM DE VOZ: Use um tom de voz **{style}**.
        
        SUA MISS√ÉO:
        Voc√™ acaba de receber as respostas iniciais do usu√°rio, que cobrem: Nome, Idade, Educa√ß√£o, Experi√™ncias Profissionais, Posi√ß√£o Atual e Objetivos.
        
        1. REVISE E VALIDE: Revise as respostas. Se alguma informa√ß√£o crucial parecer incompleta, pe√ßa esclarecimento de forma educada, mantendo o estilo de voz definido.
        2. INICIE A AN√ÅLISE: Ap√≥s a valida√ß√£o, comece a etapa 2 do PDI: 'Identificar Gaps (O que falta aprender?)'. Baseie-se nas experi√™ncias passadas e nos objetivos futuros.
        """

# Fun√ß√£o para gerar o conte√∫do usando o Gemini (mantida a l√≥gica est√°vel)
def generate_gemini_response(prompt, api_key):
    # Garante que o system prompt √© o primeiro item
    st.session_state.messages[0]['content'] = build_system_prompt()
    system_prompt = st.session_state.messages[0]['content']

    if not api_key: st.error("Erro de configura√ß√£o: A chave GEMINI_API_KEY n√£o foi encontrada.") ; return None
        
    try:
        client = genai.Client(api_key=api_key)
        history_messages = []
        for m in st.session_state.messages[1:]:
            role = 'user' if m['role'] == 'user' else 'model'
            content_obj = Content(role=role, parts=[Part.from_text(text=m['content'])]) 
            history_messages.append(content_obj)
        
        history_messages.append(Content(role='user', parts=[Part.from_text(text=prompt)]))

        response = client.models.generate_content(
            model='gemini-2.5-flash', contents=history_messages, config={'system_instruction': system_prompt} 
        )
        return response
    
    except APIError as e: st.error(f"Erro na API do Gemini: Detalhe: {e}"); return None
    except Exception as e: st.error(f"Ocorreu um erro inesperado: {e}"); return None


# Exibir mensagens anteriores no chat
for msg in st.session_state.messages:
    if msg["role"] != "system":
        role = 'assistant' if msg["role"] == 'model' else msg["role"]
        st.chat_message(role).write(msg["content"])


# --- 5. L√≥gica da M√°quina de Estados (Controle do Fluxo) ---

# Checa se o fluxo de perguntas n√£o terminou
if st.session_state.pdi_state < NUM_FLOW_STEPS:
    
    current_step = QUESTION_FLOW[st.session_state.pdi_state]
    
    # 5.1. Exibir Introdu√ß√£o
    if current_step["type"] == "intro":
        st.chat_message("assistant").write(current_step["text"])
        st.session_state.pdi_state += 1
        st.rerun()

    # 5.2. Exibir M√∫ltipla Escolha (st.radio)
    elif current_step["type"] == "select":
        st.chat_message("assistant").write(current_step["question"])
        
        # Cria um cont√™iner para posicionar o radio button e o bot√£o de avan√ßar
        with st.container():
            selected_option = st.radio("Selecione uma op√ß√£o:", current_step["options"], key=f'select_{st.session_state.pdi_state}')
            
            # Bot√£o personalizado (usa markdown para aparecer, pois o st.button est√° escondido)
            # O texto do bot√£o deve ser clic√°vel.
            if st.markdown(f'<a href="#" target="_self"><button style="color:white; background-color:#4A90E2; border-radius:5px; padding:10px;">Confirmar e Continuar</button></a>', unsafe_allow_html=True):
                # Armazena a configura√ß√£o
                st.session_state.configs[current_step["key"]] = selected_option
                
                # Registra a resposta no hist√≥rico como se fosse o usu√°rio
                st.session_state.messages.append({"role": "user", "content": f"{current_step['question']}: {selected_option}"})
                
                st.session_state.pdi_state += 1 
                st.rerun()
        
        st.stop() # Interrompe o fluxo para que o chat_input n√£o apare√ßa

    # 5.3. Exibir Pergunta de Texto (st.chat_input)
    elif current_step["type"] == "input":
        st.chat_message("assistant").write(current_step["question"])
        # A l√≥gica para capturar a resposta est√° abaixo, no st.chat_input


# 5.4. Captura a intera√ß√£o do usu√°rio (apenas para type="input")
if prompt := st.chat_input("Digite sua resposta aqui..."):
    
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    if st.session_state.pdi_state < NUM_FLOW_STEPS:
        # Aumenta o estado se a resposta foi uma entrada de texto
        st.session_state.pdi_state += 1
        
        if st.session_state.pdi_state < NUM_FLOW_STEPS:
            st.rerun() 
        else:
            # Transi√ß√£o final para o Chat Ativo
            with st.chat_message("assistant"):
                st.markdown("‚úÖ **Formul√°rio inicial completo!** O Mentor de Carreira j√° est√° analisando suas respostas. Por favor, aguarde enquanto ele processa a primeira an√°lise e inicia a fase de identifica√ß√£o de *Gaps*.")
                
            # Chama o Gemini com a √∫ltima resposta como prompt inicial
            final_prompt_to_gemini = st.session_state.messages[-1]['content']
            
            with st.chat_message("assistant"):
                response = generate_gemini_response(final_prompt_to_gemini, gemini_api_key)
                if response:
                    full_response = response.text
                    st.markdown(full_response)
                    st.session_state.messages.append({"role": "model", "content": full_response})
                else:
                    st.session_state.messages.pop()
    else:
        # 5.5. Chat Ativo (Gemini assume)
        with st.chat_message("assistant"):
            response = generate_gemini_response(prompt, gemini_api_key)
            
            if response:
                full_response = response.text
                st.markdown(full_response)
                st.session_state.messages.append({"role": "model", "content": full_response})
            else:
                st.session_state.messages.pop()
