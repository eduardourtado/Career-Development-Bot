import streamlit as st
import os # Necess√°rio para buscar a chave secreta
from google import genai
from google.genai.errors import APIError
from google.genai.types import Content, Part

# --- 1. Configura√ß√£o da Interface ---
st.set_page_config(page_title="Mentor de Carreira PDI (Gemini)", page_icon="üéØ", layout="centered")

st.title("Mentor de PDI Inteligente (Gemini)")
st.markdown("Ol√°! Sou seu assistente de carreira. Vamos construir seu **Plano de Desenvolvimento Individual** juntos. Por favor, responda o formul√°rio inicial para um planejamento eficaz.")

st.markdown("""
<style>
    .stChatMessage {
        border-radius: 10px; 
        padding: 10px;
    }
</style>
""", unsafe_allow_html=True)


# --- 2. Vari√°veis de Estado e Perguntas PERSONALIZADAS ---

# 11 Perguntas estruturadas em ordem
FORM_QUESTIONS = [
    # 1. Sobre voc√™
    "1/11. Como voc√™ preferiria que eu te chamasse?",
    "2/11. Quantos anos voc√™ tem?",
    
    # 2. Sobre experi√™ncias educacionais
    "3/11. Qual foi o maior n√≠vel de educa√ß√£o que voc√™ j√° obteve? (Op√ß√µes: Ensino Fundamental, Ensino M√©dio, Bacharelado / Licenciatura / Tecn√≥logo, P√≥s-gradua√ß√£o, M.B.A., Mestrado, Doutorado, P√≥s-doutorado, Nenhum a declarar)",
    "4/11. Em qual institui√ß√£o voc√™ obteve essa forma√ß√£o?",
    "5/11. Qual foi a sua √°rea de estudo?",
    
    # 3. Sobre experi√™ncia profissional
    "6/11. Voc√™ j√° trabalhou como jovem aprendiz? Se sim, em qual ano foi sua primeira experi√™ncia nesse formato?",
    "7/11. Voc√™ j√° trabalhou como estagi√°rio(a)? Se sim, em qual ano foi sua primeira experi√™ncia nesse formato?",
    "8/11. Voc√™ j√° trabalhou como funcion√°rio CLT? Se sim, em qual ano foi sua primeira experi√™ncia nesse formato?",
    "9/11. Por favor, cite os nomes das empresas nas quais voc√™ j√° trabalhou como CLT (separe por v√≠rgulas)",
    "10/11. Voc√™ est√° trabalhando atualmente? Se sim, cite qual √© o nome da sua posi√ß√£o e empresa atuais",
    
    # 4. Objetivos profissionais
    "11/11. Quais s√£o os seus principais objetivos profissionais?"
]
NUM_QUESTIONS = len(FORM_QUESTIONS) # Total de 11 perguntas

# --- 3. Barra Lateral para Configura√ß√£o ---
with st.sidebar:
    st.header("Configura√ß√µes")
    st.write("A chave da API est√° sendo carregada de forma segura pelo servidor (Secrets).")
    
    # O INPUT DA CHAVE FOI REMOVIDO PARA OCULTAR DO USU√ÅRIO FINAL
    
    if st.button("Limpar Conversa"):
        st.session_state.messages = []
        st.session_state.pdi_state = 0
        st.rerun()

# --- CARREGAMENTO SECRETO DA CHAVE ---
# A chave √© buscada da vari√°vel de ambiente definida no Streamlit Cloud (GEMINI_API_KEY)
gemini_api_key = os.environ.get("GEMINI_API_KEY")

# --- 4. L√≥gica de Mem√≥ria (Hist√≥rico) ---
if "messages" not in st.session_state:
    st.session_state["messages"] = [{
        "role": "system", 
        "content": """
        Voc√™ √© um Mentor de Carreira S√™nior especializado em criar Planos de Desenvolvimento Individual (PDI).
        
        SUA MISS√ÉO:
        Voc√™ acaba de receber as respostas iniciais do usu√°rio, que cobrem: Nome, Idade, Educa√ß√£o, Experi√™ncias Profissionais (Aprendiz, Est√°gio, CLT), Empresas, Posi√ß√£o Atual e Objetivos.
        
        1. REVISE E VALIDE: Revise as 11 respostas do usu√°rio. Se alguma informa√ß√£o parecer incompleta, pe√ßa esclarecimento de forma educada.
        2. INICIE A AN√ÅLISE: Ap√≥s a valida√ß√£o, comece a etapa 2 do PDI: 'Identificar Gaps (O que falta aprender?)'. Baseie-se nas experi√™ncias passadas e nos objetivos futuros.
        
        TONALIDADE: Profissional, acolhedor e focado em resultado.
        """
    }]
    # Inicializa o estado do formul√°rio
    st.session_state.pdi_state = 0 


# Fun√ß√£o para gerar o conte√∫do usando o Gemini
def generate_gemini_response(prompt, api_key):
    if not api_key:
        st.error("Erro de configura√ß√£o: A chave GEMINI_API_KEY n√£o foi encontrada no ambiente de hospedagem.")
        return None
        
    try:
        client = genai.Client(api_key=api_key)
        
        system_prompt = st.session_state.messages[0]['content']
        
        # Prepara o hist√≥rico
        history_messages = []
        for m in st.session_state.messages[1:]:
            role = 'user' if m['role'] == 'user' else 'model'
            content_obj = Content(
                role=role,
                parts=[Part.from_text(text=m['content'])] 
            )
            history_messages.append(content_obj)
        
        # Adiciona a nova mensagem do usu√°rio no formato Content
        history_messages.append(Content(role='user', parts=[Part.from_text(text=prompt)]))

        # A chamada √† API
        response = client.models.generate_content(
            model='gemini-2.5-flash', 
            contents=history_messages,
            config={'system_instruction': system_prompt} 
        )
        
        return response
    
    except APIError as e:
        st.error(f"Erro na API do Gemini: Verifique se sua chave secreta √© v√°lida e tem cr√©ditos. Detalhe: {e}")
        return None
    except Exception as e:
        st.error(f"Ocorreu um erro inesperado: {e}")
        return None


# Exibir mensagens anteriores no chat
for msg in st.session_state.messages:
    if msg["role"] != "system":
        role = 'assistant' if msg["role"] == 'model' else msg["role"]
        st.chat_message(role).write(msg["content"])


# --- 5. L√≥gica da M√°quina de Estados (Controle das 11 Perguntas) ---

# 5.1. Exibir a pr√≥xima pergunta do formul√°rio
if st.session_state.pdi_state < NUM_QUESTIONS:
    next_question = FORM_QUESTIONS[st.session_state.pdi_state]
    # O bot sempre fala a pr√≥xima pergunta no in√≠cio
    st.chat_message("assistant").write(next_question)


# 5.2. Captura a intera√ß√£o do usu√°rio
if prompt := st.chat_input("Digite sua resposta aqui..."):
    
    # Adiciona a mensagem do usu√°rio ao hist√≥rico e exibe
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # L√≥gica de Transi√ß√£o de Estado:
    if st.session_state.pdi_state < NUM_QUESTIONS:
        # Estamos no meio do formul√°rio. Aumenta o estado para a pr√≥xima pergunta.
        st.session_state.pdi_state += 1
        
        if st.session_state.pdi_state < NUM_QUESTIONS:
            # Recarrega para mostrar a pr√≥xima pergunta
            st.rerun() 
        else:
            # Transi√ß√£o para o chat ativo (Formul√°rio completo)
            
            # Exibir uma mensagem de transi√ß√£o
            with st.chat_message("assistant"):
                st.markdown("‚úÖ **Formul√°rio inicial completo!** O Mentor de Carreira j√° est√° analisando suas 11 respostas. Por favor, aguarde enquanto ele processa a primeira an√°lise e inicia a fase de identifica√ß√£o de *Gaps*.")
                
            # Na pr√≥xima execu√ß√£o, o fluxo cair√° no bloco ELSE (Chat Ativo)
            # Para for√ßar a primeira resposta do Gemini sem nova intera√ß√£o do usu√°rio:
            # Vamos reutilizar a √∫ltima resposta do usu√°rio como o prompt de ativa√ß√£o.
            final_prompt_to_gemini = st.session_state.messages[-1]['content']
            
            # Chama o Gemini com a √∫ltima resposta como prompt inicial
            with st.chat_message("assistant"):
                response = generate_gemini_response(final_prompt_to_gemini, gemini_api_key)
                if response:
                    full_response = response.text
                    st.markdown(full_response)
                    st.session_state.messages.append({"role": "model", "content": full_response})
                else:
                    st.session_state.messages.pop()
    else:
        # 5.3. Chat Ativo (Gemini)
        
        # Inicia a gera√ß√£o
        with st.chat_message("assistant"):
            response = generate_gemini_response(prompt, gemini_api_key)
            
            if response:
                full_response = response.text
                st.markdown(full_response)
                
                # Salva a resposta do bot na mem√≥ria
                st.session_state.messages.append({"role": "model", "content": full_response})
            else:
                st.session_state.messages.pop()
